#!/usr/bin/env python3
"""
Simplified training script for quantum anomaly detection.

This script trains a one-class quantum SVM on the latent representations
generated by the event generation pipeline.

Usage:
    python train_quantum_anomaly_detector.py \
        --train-file output/signal/events_for_quantum_ml.h5 \
        --test-file output/background/events_for_quantum_ml.h5 \
        --nqubits 6 \
        --output trained_models/my_model
"""

import argparse
import os
import sys
import numpy as np
import h5py
import pickle
from pathlib import Path

print("Loading dependencies...")
try:
    from sklearn.svm import OneClassSVM
    from sklearn.metrics import roc_auc_score, roc_curve
    print("✓ Scikit-learn loaded")
except ImportError as e:
    print(f"✗ Error loading scikit-learn: {e}")
    sys.exit(1)

# Optional: Try to load Qiskit (may have architecture issues on M1/M2)
QISKIT_AVAILABLE = False
try:
    from qiskit import QuantumCircuit
    from qiskit.circuit.library import ZZFeatureMap, PauliFeatureMap
    from qiskit.utils import QuantumInstance, algorithm_globals
    from qiskit_machine_learning.kernels import QuantumKernel
    from qiskit import Aer
    QISKIT_AVAILABLE = True
    print("✓ Qiskit loaded (quantum backend available)")
except ImportError as e:
    print(f"⚠ Qiskit not available: {e}")
    print("  Will use classical SVM with RBF kernel")
    QISKIT_AVAILABLE = False


def load_data(filepath):
    """Load latent space data from HDF5 file."""
    print(f"Loading data from: {filepath}")
    with h5py.File(filepath, 'r') as f:
        latent_space = f['latent_space'][:]
        # Flatten from (n_events, 2, latent_dim) to (n_events, 2*latent_dim)
        n_events = latent_space.shape[0]
        latent_dim = latent_space.shape[2]
        data = latent_space.reshape(n_events, -1)
        print(f"  Loaded {n_events} events, latent dim: {latent_dim}")
        print(f"  Flattened shape: {data.shape}")
        return data


def create_quantum_kernel(nqubits, feature_map_type='ZZFeatureMap', reps=2, backend='qasm_simulator'):
    """Create quantum kernel (if Qiskit is available)."""
    if not QISKIT_AVAILABLE:
        raise ImportError("Qiskit not available. Use classical kernel instead.")
    
    print(f"\nCreating quantum kernel:")
    print(f"  Qubits: {nqubits}")
    print(f"  Feature map: {feature_map_type}")
    print(f"  Repetitions: {reps}")
    print(f"  Backend: {backend}")
    
    # Create feature map
    if feature_map_type == 'ZZFeatureMap':
        feature_map = ZZFeatureMap(feature_dimension=nqubits, reps=reps)
    elif feature_map_type == 'PauliFeatureMap':
        feature_map = PauliFeatureMap(feature_dimension=nqubits, reps=reps)
    else:
        raise ValueError(f"Unknown feature map: {feature_map_type}")
    
    # Create quantum instance
    if backend == 'statevector_simulator':
        backend_instance = Aer.get_backend('statevector_simulator')
    else:
        backend_instance = Aer.get_backend('qasm_simulator')
    
    quantum_instance = QuantumInstance(backend_instance, shots=1024)
    
    # Create quantum kernel
    quantum_kernel = QuantumKernel(feature_map=feature_map, quantum_instance=quantum_instance)
    
    print("✓ Quantum kernel created")
    return quantum_kernel


def train_classical_svm(train_data, nu=0.05, gamma='scale'):
    """Train classical one-class SVM with RBF kernel."""
    print(f"\nTraining classical one-class SVM:")
    print(f"  Training samples: {len(train_data)}")
    print(f"  Nu parameter: {nu}")
    print(f"  Gamma: {gamma}")
    
    model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)
    model.fit(train_data)
    
    print("✓ Classical SVM trained")
    return model


def train_quantum_svm(train_data, quantum_kernel, nu=0.05):
    """Train quantum one-class SVM with precomputed kernel."""
    print(f"\nTraining quantum one-class SVM:")
    print(f"  Training samples: {len(train_data)}")
    print(f"  Nu parameter: {nu}")
    print("  Computing quantum kernel matrix...")
    
    # Compute kernel matrix
    kernel_matrix = quantum_kernel.evaluate(x_vec=train_data)
    
    print(f"  Kernel matrix shape: {kernel_matrix.shape}")
    print("  Training SVM with precomputed kernel...")
    
    model = OneClassSVM(kernel='precomputed', nu=nu)
    model.fit(kernel_matrix)
    
    # Store kernel and data for prediction
    model.quantum_kernel = quantum_kernel
    model.train_data = train_data
    
    print("✓ Quantum SVM trained")
    return model


def evaluate_model(model, test_data, test_labels, is_quantum=False):
    """Evaluate model on test data."""
    print(f"\nEvaluating model:")
    print(f"  Test samples: {len(test_data)}")
    
    if is_quantum:
        # Compute kernel matrix between test and training data
        kernel_matrix = model.quantum_kernel.evaluate(x_vec=test_data, y_vec=model.train_data)
        predictions = model.predict(kernel_matrix)
        scores = model.decision_function(kernel_matrix)
    else:
        predictions = model.predict(test_data)
        scores = model.decision_function(test_data)
    
    # Calculate metrics
    accuracy = np.mean(predictions == test_labels)
    
    # For anomaly detection, we want higher scores for inliers (signal)
    # and lower scores for outliers (background)
    if len(np.unique(test_labels)) > 1:
        auc = roc_auc_score(test_labels, scores)
    else:
        auc = None
    
    print(f"✓ Accuracy: {accuracy:.4f}")
    if auc is not None:
        print(f"✓ ROC AUC: {auc:.4f}")
    
    return {
        'predictions': predictions,
        'scores': scores,
        'accuracy': accuracy,
        'auc': auc
    }


def save_model(model, output_path, results=None, is_quantum=False):
    """Save trained model and results."""
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nSaving model to: {output_path}")
    
    # Save model
    model_file = output_path / 'model.pkl'
    with open(model_file, 'wb') as f:
        pickle.dump(model, f)
    print(f"  ✓ Model saved: {model_file}")
    
    # Save results
    if results:
        results_file = output_path / 'results.pkl'
        with open(results_file, 'wb') as f:
            pickle.dump(results, f)
        print(f"  ✓ Results saved: {results_file}")
    
    # Save metadata
    metadata = {
        'is_quantum': is_quantum,
        'model_type': 'Quantum SVM' if is_quantum else 'Classical SVM',
        'kernel': 'quantum' if is_quantum else 'rbf'
    }
    metadata_file = output_path / 'metadata.pkl'
    with open(metadata_file, 'wb') as f:
        pickle.dump(metadata, f)
    print(f"  ✓ Metadata saved: {metadata_file}")


def main():
    parser = argparse.ArgumentParser(
        description='Train quantum anomaly detector',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--train-file', required=True,
                       help='Path to training data (signal) HDF5 file')
    parser.add_argument('--test-file', default=None,
                       help='Path to test data (background) HDF5 file')
    parser.add_argument('--nqubits', type=int, default=6,
                       help='Number of qubits (must match latent dimension)')
    parser.add_argument('--feature-map', default='ZZFeatureMap',
                       choices=['ZZFeatureMap', 'PauliFeatureMap'],
                       help='Quantum feature map type')
    parser.add_argument('--reps', type=int, default=2,
                       help='Feature map repetitions')
    parser.add_argument('--backend', default='qasm_simulator',
                       choices=['qasm_simulator', 'statevector_simulator'],
                       help='Quantum backend')
    parser.add_argument('--nu-param', type=float, default=0.05,
                       help='Nu parameter for one-class SVM')
    parser.add_argument('--gamma', default='scale',
                       help='Gamma for classical RBF kernel')
    parser.add_argument('--classical', action='store_true',
                       help='Force use of classical SVM (RBF kernel)')
    parser.add_argument('--output', default='trained_models/model',
                       help='Output directory for trained model')
    parser.add_argument('--ntrain', type=int, default=None,
                       help='Number of training samples (use subset)')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("QUANTUM ANOMALY DETECTION TRAINING")
    print("=" * 80)
    
    # Load training data (signal)
    train_data = load_data(args.train_file)
    
    # Check dimensions
    expected_nqubits = train_data.shape[1]
    if args.nqubits != expected_nqubits:
        print(f"\n⚠ WARNING: nqubits={args.nqubits} but data has {expected_nqubits} features")
        print(f"  Adjusting nqubits to {expected_nqubits}")
        args.nqubits = expected_nqubits
    
    # Use subset if requested
    if args.ntrain and args.ntrain < len(train_data):
        print(f"\nUsing subset of training data: {args.ntrain} samples")
        train_data = train_data[:args.ntrain]
    
    # Decide whether to use quantum or classical
    use_quantum = QISKIT_AVAILABLE and not args.classical
    
    if use_quantum:
        print("\n→ Using QUANTUM kernel")
        quantum_kernel = create_quantum_kernel(
            args.nqubits,
            args.feature_map,
            args.reps,
            args.backend
        )
        model = train_quantum_svm(train_data, quantum_kernel, args.nu_param)
        is_quantum = True
    else:
        print("\n→ Using CLASSICAL RBF kernel")
        if not QISKIT_AVAILABLE:
            print("  (Qiskit not available)")
        model = train_classical_svm(train_data, args.nu_param, args.gamma)
        is_quantum = False
    
    # Evaluate if test data provided
    results = None
    if args.test_file:
        test_data = load_data(args.test_file)
        
        # Use subset if requested
        if args.ntrain:
            test_data = test_data[:args.ntrain]
        
        # Labels: 1 for signal (inliers), -1 for background (outliers)
        train_labels = np.ones(len(train_data))
        test_labels = -np.ones(len(test_data))
        
        # Combine for evaluation
        all_data = np.vstack([train_data, test_data])
        all_labels = np.hstack([train_labels, test_labels])
        
        results = evaluate_model(model, all_data, all_labels, is_quantum)
    
    # Save model
    save_model(model, args.output, results, is_quantum)
    
    print("\n" + "=" * 80)
    print("TRAINING COMPLETED SUCCESSFULLY")
    print("=" * 80)
    print(f"\nModel saved to: {args.output}")
    print("\nNext steps:")
    print(f"  1. Use the trained model for predictions")
    print(f"  2. Visualize results with analysis scripts")
    print("=" * 80)


if __name__ == "__main__":
    main()
